# KB-RAG - AI Knowledge Base with RAG

A production-ready knowledge base system using Retrieval-Augmented Generation (RAG) with FastAPI, LangChain, and Google Gemini 2.0 Flash.

###DEMO VIDEO:
https://github.com/user-attachments/assets/ac97a2ac-840f-4877-b587-85937097f7c5
---

## âœ¨ Key Features

- **Multi-Document Support**: Upload multiple PDFs, TXT, or MD files and query across all documents
- **Smart Retrieval**: Automatically uses ALL document chunks for maximum accuracy
- **Latest AI Model**: Google Gemini 2.0 Flash Experimental with enhanced prompt engineering
- **Chat History**: Persistent conversation history with search functionality
- **Free to Use**: Google Gemini (generous free tier) + HuggingFace embeddings (completely free)
- **Clean UI**: Netflix-style interface with collapsible sidebar and drag-drop upload
- **Production Ready**: Optimized for Render.com deployment with memory management

## ğŸš€ Quick Start

### 1. Setup Environment

```bash
# Create and activate virtual environment
python -m venv .venv
.\.venv\Scripts\activate  # Windows
source .venv/bin/activate  # Linux/Mac

# Install dependencies
pip install -r requirements.txt
```

### 2. Configure API Key

Create a `.env` file:
```
GOOGLE_API_KEY=your-gemini-api-key-here
```

Get your free API key from: https://makersuite.google.com/app/apikey

### 3. Run Application

```bash
uvicorn app.main:app --reload --port 8000
```

Visit: http://localhost:8000

## ğŸ“– Usage

1. **Upload Documents**: Drag and drop multiple PDF, TXT, or MD files
2. **Ask Questions**: Type questions about your uploaded documents
3. **View Sources**: Each answer includes source document references
4. **Clear Knowledge Base**: Use "Clear All" button to reset and start fresh

## ğŸ—ï¸ Architecture

```
User Query â†’ FastAPI â†’ FAISS Retriever â†’ LangChain â†’ Gemini 2.0 â†’ Synthesized Answer
                â†“                                                           â†“
         Vector Store (All Chunks)                              Source Attribution
```

**RAG Pipeline:**
- **Chunking**: 800 chars with 150 char overlap using RecursiveCharacterTextSplitter
- **Embeddings**: HuggingFace sentence-transformers/all-MiniLM-L6-v2 (384 dim)
- **Vector DB**: FAISS for fast semantic search
- **LLM**: Google Gemini 2.0 Flash (temperature=0.0 for consistency)
- **Retrieval**: Automatic ALL-chunk retrieval for maximum accuracy

## ğŸ“ Project Structure

```
kb-rag/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py          # FastAPI app & endpoints
â”‚   â”œâ”€â”€ ingest.py        # Document processing & vector store
â”‚   â”œâ”€â”€ query.py         # RAG query engine
â”‚   â”œâ”€â”€ storage.py       # Storage management
â”‚   â””â”€â”€ utils.py         # File parsing utilities
â”œâ”€â”€ static/
â”‚   â”œâ”€â”€ css/            # Stylesheets
â”‚   â””â”€â”€ js/             # Frontend logic
â”œâ”€â”€ templates/
â”‚   â”œâ”€â”€ index.html      # Home page
â”‚   â””â”€â”€ app.html        # Main application
â”œâ”€â”€ requirements.txt    # Python dependencies
â”œâ”€â”€ .env               # Environment variables
â””â”€â”€ README.md
```

## ğŸ”§ API Endpoints

- `GET /` - Home page
- `GET /app` - Main application interface
- `POST /ingest` - Upload and process documents
- `POST /query` - Query the knowledge base
- `POST /clear` - Clear all documents from vector store
- `GET /health` - Health check endpoint

## ğŸ¯ Technical Highlights

- **Singleton Pattern**: Embeddings model loaded once for performance
- **Memory Optimized**: Batch processing with garbage collection for 512MB limit
- **Smart Chunking**: Hierarchical text splitting preserves context
- **Error Handling**: Comprehensive try-except blocks with detailed logging
- **Lazy Loading**: Heavy imports only when needed for faster startup

## ğŸš€ Deployment

Configured for Render.com with:
- `/tmp` storage for ephemeral filesystem
- Uvicorn with 180s timeout
- Health check endpoint for monitoring
- Memory-efficient batch processing

## ğŸ“Š Performance

- Embedding Model Load: ~10-30s (first time only)
- Document Upload: ~2-5s per document
- Query Response: ~2-4s with source attribution
- Vector Search: <1s for 100 chunks

## ğŸ¤ Contributing

Contributions welcome! This project demonstrates:
- Production-ready RAG implementation
- Multi-document knowledge base management
- Modern web UI with persistent chat history
- Cloud deployment best practices
